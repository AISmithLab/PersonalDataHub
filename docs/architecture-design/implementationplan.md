# PersonalDataHub Implementation Plan

Step-by-step plan for building PersonalDataHub V1. Each step is self-contained, builds on previous steps, and includes validation criteria.

---

## Step 1: Project Scaffolding

**What:** Initialize the TypeScript project with build tooling, linting, and test framework.

**Implement:**
- `pnpm init`, install TypeScript, vitest, `@types/node`
- `tsconfig.json` with strict mode, ESM output, path aliases
- `package.json` scripts: `build`, `dev`, `test`, `lint`
- `src/index.ts` entry point (just a "hello world" console log for now)
- `.gitignore`, `.eslintrc`

**Validate:**
- `pnpm build` compiles with zero errors
- `pnpm test` runs vitest and reports 0 tests (no failures)
- `node dist/index.js` prints a startup message

---

## Step 2: Configuration System

**What:** Load and validate `hub-config.yaml` using Zod. This is the foundation — almost everything reads from config.

**Implement:**
- `src/config/types.ts` — TypeScript types for the full config shape (sources, boundary, cache, owner_auth, agent_identity)
- `src/config/schema.ts` — Zod schema matching the config types. Validate source names, boundary fields, optional cache block, credential placeholders
- `src/config/loader.ts` — Load YAML file from a path, parse with `yaml`, validate with Zod, resolve `${ENV_VAR}` placeholders from environment
- `hub-config.example.yaml` — Example config with both Gmail and GitHub sources (matching design.md)

**Dependencies:** `yaml`, `zod`

**Validate:**
- Unit test: valid config YAML parses and returns typed object
- Unit test: invalid config (missing required fields, bad types) throws `ZodError` with clear message
- Unit test: `${ENV_VAR}` placeholders resolve from `process.env`
- Unit test: disabled source (`enabled: false`) is accepted but skipped
- Unit test: config with `cache` block parses correctly; config without `cache` block defaults to `{ enabled: false }`

---

## Step 3: Database and Schema

**What:** SQLite database with 5 tables. Access control is manifest-based: each API key maps to a set of allowed manifests. The Hub binds to localhost only. Encryption at rest for cached data.

**Tables:**

1. **`api_keys`** — lightweight access control. Owner generates a key per app, each key is allowed to execute a set of manifests.

   | Column | Type | Description |
   |---|---|---|
   | `id` | string | Key identifier (e.g., `"openclaw"`) |
   | `key_hash` | string | bcrypt hash of the API key |
   | `name` | string | Display name (e.g., `"OpenClaw Agent"`) |
   | `allowed_manifests` | json | `["email-search", "github-issues"]` or `["*"]` for all |
   | `enabled` | boolean | Kill switch |
   | `created_at` | datetime | |

2. **`manifests`** — the access control layer. Generated by the GUI from the owner's access control settings. Each manifest defines exactly what data can be accessed and how.

   | Column | Type | Description |
   |---|---|---|
   | `id` | string | Manifest name (e.g., `"gmail-readonly-recent"`) |
   | `source` | string | Which source this manifest applies to |
   | `purpose` | string | From `@purpose` |
   | `raw_text` | text | Full manifest source (generated by GUI) |
   | `status` | string | `"active"`, `"inactive"` |
   | `created_at` | datetime | |
   | `updated_at` | datetime | |

3. **`cached_data`** — local cache of source data, encrypted at rest. Schema-flexible: raw data stored as a JSON blob so any source type works without schema changes.

   | Column | Type | Description |
   |---|---|---|
   | `id` | string | Hub-internal ID |
   | `source` | string | `"gmail"`, `"github"`, etc. (indexed for queries) |
   | `source_item_id` | string | Original ID in source system (indexed, unique per source) |
   | `type` | string | `"email"`, `"issue"`, `"pr"`, `"commit"`, etc. (indexed) |
   | `timestamp` | datetime | Item timestamp (indexed for boundary `after` filtering) |
   | `data` | text | Raw JSON blob containing the full DataRow fields. **Encrypted at rest.** |
   | `cached_at` | datetime | When this row was cached |
   | `expires_at` | datetime | TTL-based expiration |

   The `source`, `type`, and `timestamp` columns are kept separate for indexed querying (`pull` filters on these). All content (title, body, author, labels, etc.) lives inside `data` as JSON. The connector's mapper deserializes `data` → `DataRow` at read time.

4. **`staging`** — outbound actions pending owner review. Used for sources requiring outbound review (e.g., Gmail where the owner reviews email drafts before sending). Not used for GitHub, where the agent acts directly with its own scoped credentials.

   | Column | Type | Description |
   |---|---|---|
   | `action_id` | string | Hub-generated ID |
   | `manifest_id` | string | Which manifest authorized this |
   | `source` | string | Target service |
   | `action_type` | string | `"send_email"`, `"comment_on_issue"`, etc. |
   | `action_data` | json | All data for the action: parameters, context, what triggered it |
   | `purpose` | string | Why the app proposed this action (from request) |
   | `status` | string | `"pending"`, `"approved"`, `"rejected"`, `"committed"` |
   | `proposed_at` | datetime | |
   | `resolved_at` | datetime | |

5. **`audit_log`** — append-only log of all data movements.

   | Column | Type | Description |
   |---|---|---|
   | `id` | integer | Auto-increment |
   | `timestamp` | datetime | |
   | `event` | string | `"data_pull"`, `"cache_write"`, `"action_proposed"`, `"action_approved"`, `"action_rejected"`, `"action_committed"` |
   | `source` | string | Which source was involved |
   | `details` | json | Event-specific data (graph, operators, results count, etc.) |

**Access control flow:**
```
Request: POST /app/v1/pull { source: "gmail", purpose: "Find emails about Q4 report" }
  Header: Authorization: Bearer pk_xxx
  → Hub verifies: is pk_xxx a valid, enabled key? (api_keys table)
  → Hub resolves policy: find approved manifest for this key + source
  → Execute pipeline, log purpose to audit, return data
```

**Implement:**
- `src/db/db.ts` — Create or open a SQLite database file using `better-sqlite3`. Export a `getDb(configPath)` function. Handle WAL mode for concurrency. Bind to localhost only.
- `src/db/schema.ts` — Create all 5 tables on first run. SQL CREATE statements for each table above.
- `src/db/encryption.ts` — Application-level encryption using Node.js `crypto` (AES-256-GCM) for the `data` column in `cached_data`. Encryption key derived from a master secret in config or environment variable.

**Dependencies:** `better-sqlite3`, `@types/better-sqlite3`

**Validate:**
- Unit test: `getDb()` creates a new database file with all 5 tables
- Unit test: each table has the correct columns and types
- Unit test: `encryptField()` → `decryptField()` round-trips correctly
- Unit test: insert and read from each table
- Unit test: `api_keys` — insert a key, look up by hash, verify `allowed_manifests` JSON parses
- Unit test: `audit_log` — append-only, entries auto-increment
- Manual: inspect the `.db` file with `sqlite3` CLI to verify schema

---

## Step 4: Normalized Data Shape and Connector Interface

**What:** Define the `DataRow` type (minimal fixed fields + flexible data map), the `SourceConnector` interface, and serialization helpers. The data model is designed to support any future source without schema changes.

**Implement:**
- `src/connectors/types.ts`:
  - `DataRow` type — minimal fixed fields + flexible data map:
    ```typescript
    type DataRow = {
      source: string;          // "gmail", "github", "slack", "gcal", etc.
      source_item_id: string;  // original ID in source system
      type: string;            // "email", "issue", "pr", "commit", "event", etc.
      timestamp: string;       // ISO 8601
      data: Record<string, unknown>;  // all content fields, flexible per source
    }
    ```
    The 4 fixed fields (`source`, `source_item_id`, `type`, `timestamp`) match the indexed columns in `cached_data`. Everything else lives in `data` — each connector defines its own field names. Operators (`select`, `filter`, `transform`) work on keys within `data`.
  - `SourceConnector` interface:
    - `name: string` — connector name ("gmail", "github")
    - `fetch(boundary, params): Promise<DataRow[]>` — fetch data from source API within boundary
    - `executeAction(actionType, actionData): Promise<ActionResult>` — execute an approved outbound action
  - `ActionResult` type: `{ success: boolean, message: string, resultData?: Record<string, unknown> }`
  - `ConnectorRegistry` — map of source name → connector instance
  - `serializeDataRow(row: DataRow): string` — serialize to JSON for `cached_data.data` column
  - `deserializeDataRow(json: string): DataRow` — parse back from cache

  Example `data` fields per source:
  - **Gmail**: `{ title, body, author_name, author_email, participants, labels, url, attachments, threadId, isUnread, snippet }`
  - **GitHub issue**: `{ title, body, author_name, author_url, labels, url, repo, number, state }`
  - **GitHub commit**: `{ title, body, author_name, url, repo, sha, additions, deletions, changedFiles }`
  - **Calendar** (future): `{ title, start_time, end_time, location, attendees, recurrence }`
  - **Slack** (future): `{ channel, thread_id, message_text, sender, reactions }`

**Validate:**
- Unit test: `DataRow` with Gmail `data` fields type-checks and serializes correctly
- Unit test: `DataRow` with GitHub `data` fields type-checks and serializes correctly
- Unit test: `serializeDataRow` → `deserializeDataRow` round-trips correctly
- Unit test: `data` map can hold any shape — nested objects, arrays, strings, numbers
- Type-level: a mock connector implementing `SourceConnector` compiles without errors

---

## Step 5: Manifest Parser

**What:** Parse `.manifest` files into a structured AST. This is a standalone module with no external dependencies.

**Implement:**
- `src/manifest/types.ts`:
  - `Manifest { id, purpose, graph: string[], operators: Map<string, OperatorDecl> }`
  - `OperatorDecl { name, type, properties: Record<string, unknown> }`
- `src/manifest/parser.ts`:
  - Line-oriented parser: read `@purpose`, `@graph`, skip comments (`//`), parse operator lines (`name: type { props }`)
  - Property values: strings (quoted), numbers, booleans, arrays (JSON-like `["a", "b"]`)
  - Return a `Manifest` object
- `src/manifest/validator.ts`:
  - Validate that every node in `@graph` has a corresponding operator declaration
  - Validate that operator types are in the known set (`pull`, `select`, `filter`, `transform`, `stage`, `store`)
  - Validate that `@graph` forms a valid DAG (no cycles — for V1, just check it's a linear chain since we don't support branching yet)
  - Validate required properties per operator type (e.g., `pull` requires `source`)

**Validate:**
- Unit test: parse the `email-search.manifest` from design.md → correct purpose, 4-node graph, correct properties on each operator
- Unit test: parse the `github-issues.manifest` → correct structure
- Unit test: parse the `propose-email-reply.manifest` (single-node graph) → correct
- Unit test: parser rejects manifest with missing `@purpose`
- Unit test: parser rejects manifest with unknown operator type
- Unit test: validator rejects graph referencing an undeclared operator name
- Unit test: validator rejects `pull` operator without `source` property
- Unit test: comments and blank lines are ignored

---

## Step 6: Operator Framework and V1 Operators (6 total)

**What:** Implement the 6 V1 operators and the operator registry.

**Operators:** `pull`, `select`, `filter`, `transform`, `stage`, `store`

**Implement:**
- `src/operators/types.ts`:
  - `Operator` interface: `execute(input: DataRow[], context: PipelineContext, props: Record<string, unknown>): Promise<DataRow[] | ActionResult | void>`
  - `PipelineContext`: holds `db` handle, `connectorRegistry`, `boundary` config, `auditLog` reference
- `src/operators/registry.ts`:
  - Map of operator type name → operator implementation
  - `getOperator(type: string): Operator`
- `src/operators/pull.ts`:
  - Read `source` and `type` from properties
  - **Lazy loading**: check `cached_data` table first (WHERE source, type, timestamp match boundary). If cache has fresh, sufficient data → return from cache. If cache is empty, stale, or doesn't cover the requested range → fetch from source API via connector, return results.
  - Look up the connector from `context.connectorRegistry`
  - Call `connector.fetch(boundary, { type, time_window })` only when cache is insufficient
- `src/operators/select.ts`:
  - Read `fields` array from properties
  - Return rows with only the specified fields kept in `data` (drop all other keys from `data`)
- `src/operators/filter.ts`:
  - Read `field`, `op`, `value` from properties
  - Fields refer to keys in `data` (e.g., `field: "labels"` → `row.data.labels`)
  - Supported ops: `eq`, `neq`, `contains`, `gt`, `lt`, `matches` (regex)
  - Drop rows that don't match
- `src/operators/transform.ts`:
  - Read `kind` from properties
  - Fields refer to keys in `data`
  - `kind: "redact"` — regex replace on a field in `data`
  - `kind: "truncate"` — truncate a field in `data` to `max_length` chars
- `src/operators/stage.ts`:
  - Read `action_type`, `requires_approval` from properties
  - Insert a row into the `staging` table with status `pending`
  - Return an `ActionResult` with the `action_id`
  - Used for sources requiring outbound review (e.g., Gmail). Not used for GitHub, where the agent acts directly with its own credentials.
- `src/operators/store.ts`:
  - Write input `DataRow[]` to the `cached_data` table (encrypted at rest)
  - Upsert by `(source, source_item_id)` — if the row already exists, update it
  - Set `cached_at` to now, `expires_at` based on config TTL
  - Used in "loading" manifests to explicitly pre-populate the local cache:
    ```
    @purpose: "Load and cache recent emails locally"
    @graph: pull_emails -> store_locally
    pull_emails: pull { source: "gmail", type: "email" }
    store_locally: store { }
    ```
  - Subsequent `pull` calls will find this data in cache (lazy loading) and skip the API call

**Validate:**
- Unit test `pull` (cache miss): with empty cache and mock connector, fetches from connector, returns rows
- Unit test `pull` (cache hit): with pre-populated cache, returns from cache, does NOT call connector
- Unit test `pull` (lazy loading): cache has data but doesn't cover requested time range → fetches only the missing range from connector
- Unit test `select`: given 3 rows with 5 fields each in `data`, `select { fields: ["title", "body"] }` returns 3 rows with only 2 fields in `data`
- Unit test `filter`: given rows with labels in `data`, `filter { field: "labels", op: "contains", value: "bug" }` keeps only matching rows
- Unit test `filter`: `op: "eq"` on string field works
- Unit test `filter`: `op: "matches"` with regex works
- Unit test `transform` redact: replaces SSN pattern with `[REDACTED]` in `data.body`
- Unit test `transform` truncate: truncates `data.body` to specified length, appends `...`
- Unit test `stage`: inserts a row into staging table with correct fields and `pending` status
- Unit test `store`: writes DataRows to `cached_data` table with correct fields, encrypted `data` column
- Unit test `store`: upsert — writing same `source_item_id` twice updates the row, doesn't duplicate
- Unit test `store` + `pull`: `store` writes rows → subsequent `pull` returns them from cache

---

## Step 7: Pipeline Engine

**What:** Assemble a manifest's `@graph` into a chain of operators and execute them in sequence. The engine must handle three kinds of operators: producers (`pull`), transformers (`select`, `filter`, `transform`), side-effect operators (`store`, `stage`).

**Implement:**
- `src/pipeline/context.ts`:
  - `PipelineContext` class: holds all runtime state needed by operators (appId, db, connectors, config, boundary, auditLog handle)
- `src/pipeline/engine.ts`:
  - `executePipeline(manifest: Manifest, context: PipelineContext, params?: Record<string, unknown>): Promise<PipelineResult>`
  - Walk the `@graph` nodes in order
  - For each node, look up the operator from registry, look up properties from the manifest's operator declarations
  - Pass output of previous operator as input to the next
  - First operator (`pull`) produces the initial `DataRow[]`
  - Last operator's output is the pipeline result
  - Track execution metadata: operators applied, items fetched, items returned, query time
  - **Operator return type handling:**
    - `pull`, `select`, `filter`, `transform` → return `DataRow[]`, passed to next operator
    - `store` → writes rows to `cached_data`, then **passes the same `DataRow[]` through** to the next operator (pass-through side effect). If `store` is the last node in the graph, the pipeline result is the stored rows.
    - `stage` → inserts into `staging` table, returns `ActionResult`. Always terminal — must be the last node in the graph. Engine should reject manifests where `stage` is not the final operator.

**Validate:**
- Unit test: full pipeline with mock connector — `pull -> select -> transform(redact)` returns correct transformed rows
- Unit test: pipeline with `filter` — input 10 rows, filter keeps 3, select narrows fields, output is 3 rows with fewer fields
- Unit test: pipeline with `stage` — writes to staging table, returns action ID
- Unit test: pipeline with `store` as terminal — `pull -> store` writes rows to `cached_data` and returns the stored rows
- Unit test: pipeline with `store` mid-chain — `pull -> store -> select -> filter` writes rows to cache AND passes them through to `select`, final output is filtered/selected subset
- Unit test: pipeline metadata reports correct `operatorsApplied` and `itemsFetched`/`itemsReturned` counts
- Unit test: pipeline with unknown operator name in graph throws clear error
- Unit test: empty result from `pull` (no matching data) propagates through pipeline without error, returns empty array
- Unit test: manifest with `stage` not as the last operator → engine rejects with clear error

---

## Step 8: Audit Log

**What:** Record every data movement (queries, cache writes, proposed actions, approvals, rejections) to an append-only log. Event names and shapes match the design.md Audit Log section.

**Events** (from design.md):

| Event | When | Key details |
|---|---|---|
| `data_pull` | App pulls data | `source`, `purpose`, `resultsReturned`, `initiatedBy` (e.g., `"app:openclaw"`) |
| `cache_write` | `store` operator writes rows to `cached_data` | `source`, `rowsWritten`, `initiatedBy` |
| `action_proposed` | App proposes an outbound action | `actionId`, `source`, `action_type`, `purpose`, `initiatedBy` |
| `action_approved` | Owner approves a staged action | `actionId`, `initiatedBy` (`"owner"`) |
| `action_rejected` | Owner rejects a staged action | `actionId`, `initiatedBy` (`"owner"`) |
| `action_committed` | Approved action executed via connector | `actionId`, `source`, `result` (`"success"` / `"failure"`) |

**Implement:**
- `src/audit/log.ts`:
  - `AuditLog` class wrapping the `audit_log` SQLite table (schema from Step 3: `id`, `timestamp`, `event`, `manifest_id`, `details` JSON)
  - Write methods — each inserts one row with the correct `event` string and `details` JSON:
    - `logPull(source, purpose, resultsReturned, initiatedBy)`
    - `logCacheWrite(source, rowsWritten, initiatedBy)`
    - `logActionProposed(actionId, source, actionType, purpose, initiatedBy)`
    - `logActionApproved(actionId, initiatedBy)`
    - `logActionRejected(actionId, initiatedBy)`
    - `logActionCommitted(actionId, source, result)`
  - Read method:
    - `getEntries(filters?: { after?, before?, event?, source?, limit? }): AuditEntry[]`
  - All timestamps are ISO 8601 UTC

**Validate:**
- Unit test: `logPull` creates an entry with event `"data_pull"` and `details.purpose` stored correctly
- Unit test: `logActionProposed` creates entry with `details.purpose` and `details.action_type`
- Unit test: `logActionProposed` → `logActionApproved` → `logActionCommitted` creates 3 entries in order
- Unit test: `logActionRejected` creates entry with event `"action_rejected"`
- Unit test: `getEntries({ event: "data_pull" })` filters correctly
- Unit test: `getEntries({ after: "2026-02-20" })` filters by time
- Unit test: entries are append-only (no update/delete methods exposed)

---

## Step 9: HTTP Server and App API

**What:** Hono HTTP server bound to `127.0.0.1` with **2 App API endpoints**. Apps send requests with a `purpose` string describing why they need the data. The Hub resolves the policy (configured via GUI) and logs the purpose in the audit log. Apps never see manifests or policies.

**Implement:**
- `src/server/server.ts`:
  - Create Hono app, bind to `127.0.0.1`, start with `@hono/node-server`
  - Mount app routes under `/app/v1`
- `src/server/app-api.ts` — 2 endpoints, both require `Authorization: Bearer pk_xxx` (verified against `api_keys` table inline):

  1. **`POST /app/v1/pull`** — pull data from a source
     - Accept: `{ source, type?, params?, purpose }`
       - `purpose`: human-readable string describing why the app needs this data (e.g., `"Collect unanswered emails to draft responses"`)
     - Verify API key → resolve policy for this key+source → execute pipeline → log purpose to audit → return data
     - Return: `{ ok: true, data: DataRow[] }`

  2. **`POST /app/v1/propose`** — propose an outbound action (staged for owner review)
     - Accept: `{ source, action_type, action_data, purpose }`
       - `purpose`: why the app is proposing this action (e.g., `"Draft reply to unanswered email from Alice"`)
     - Verify API key → check policy allows this action type → insert into `staging` table with status `"pending"` → log purpose to audit
     - Return: `{ ok: true, actionId: "act_def456", status: "pending_review" }`

**Dependencies:** `hono`, `@hono/node-server`

**Validate:**
- Integration test: `POST /app/v1/pull` without valid API key → `401`
- Integration test: `POST /app/v1/pull` with valid key and purpose → returns `{ ok, data }`, audit log contains the purpose
- Integration test: `POST /app/v1/propose` with valid key and purpose → creates staging row, audit log contains the purpose
- Integration test: `POST /app/v1/pull` without purpose → `400`

---

## Step 10: GitHub Connector

**What:** PersonalDataHub's role for GitHub is **access control only** — it manages which repositories the agent's GitHub account can access. The agent works with GitHub directly using its own credentials. PersonalDataHub does not fetch or proxy GitHub data; it controls the boundary (which repos, what permission level).

**Implement:**
- `src/connectors/github/connector.ts`:
  - Implement `SourceConnector` interface
  - `getAccessList(boundary)`: return the list of repos the agent is allowed to access, based on the owner's boundary config (`boundary.repos`)
  - `validateAccess(repo)`: check if a repo is within the allowed boundary
  - No `fetch` of actual GitHub data — the agent reads/writes GitHub directly with its own PAT
  - No `executeAction` — the agent acts on GitHub with its own credentials
- `src/connectors/github/setup.ts`:
  - Use owner's GitHub PAT (via Octokit) to manage collaborator access on repos
  - `grantAccess(agentUsername, repo, permission)` — add agent account as collaborator with specified permission (e.g., read, write)
  - `revokeAccess(agentUsername, repo)` — remove agent account from repo
  - `listGrantedRepos(agentUsername)` — list repos the agent currently has access to

**Dependencies:** `octokit`

**Validate:**
- Unit test: `validateAccess("myorg/frontend")` returns true when repo is in boundary
- Unit test: `validateAccess("myorg/billing")` returns false when repo is not in boundary
- Unit test: `grantAccess` with mock Octokit calls the correct collaborator API
- Unit test: `revokeAccess` with mock Octokit calls the correct remove collaborator API
- Unit test: `listGrantedRepos` returns only repos where agent has been added

---

## Step 11: Gmail Connector

**What:** Gmail is a true data connector — unlike GitHub (access control only), PersonalDataHub fetches and mediates Gmail data on behalf of the owner. Two responsibilities: (1) pull emails via `POST /app/v1/pull`, (2) execute approved outbound actions (send/reply/draft) after owner approves via GUI.

**Implement:**
- `src/connectors/gmail/connector.ts`:
  - Implement `SourceConnector` interface
  - `fetch(boundary, params)`: query Gmail API via googleapis
    - Respect `boundary.after` — only fetch emails after the cutoff date
    - Respect optional `boundary.labels` and `boundary.exclude_labels`
    - Build Gmail search query string from boundary + params
    - Map raw messages → `DataRow[]` (all fields in `data: Record<string, unknown>`)
    - `DataRow.data` fields: `title`, `body` (plaintext, HTML stripped), `author_name`, `author_email`, `participants`, `labels`, `attachments`, `threadId`, `isUnread`, `snippet`
  - `executeAction(actionType, actionData)`: called when owner approves a staged action via GUI
    - `send_email`: compose MIME message, send via Gmail API
    - `reply_email`: reply to a thread
    - `draft_email`: create a draft
  - `sync()`: fetch latest emails since last sync, write to `cached_data` via `store` logic (upsert by `source_item_id`). Called on a schedule to keep local cache fresh.
- **Scheduling**: on server startup (`pdh serve`), set up `setInterval` to call `sync()` at the configured interval (default: 5 minutes). Interval is configurable in `hub-config.yaml` under `sources.gmail.sync_interval`.

**Dependencies:** `googleapis`

**Validate:**
- Unit test: raw Gmail API message → correct `DataRow` with all fields in `data`
- Unit test: `fetch` with mock googleapis builds correct query string from boundary
- Unit test: `executeAction("send_email", ...)` calls correct Gmail API method
- Unit test: `sync()` fetches only emails since last sync timestamp, writes to `cached_data`
- Unit test: `sync()` called twice — second call only fetches new emails, doesn't duplicate

---

## Step 12: Owner GUI

**What:** Local web GUI for the owner to manage PersonalDataHub. Tab-based layout (like Excel tabs) — each connected service gets its own tab. Served by the same Hono server from Step 9. Reads/writes DB directly.

**Layout:**
- Top-level navigation: tabs for each service (e.g., **Gmail**, **GitHub**) + a **Settings** tab
- Each service tab starts disconnected. The owner connects by logging in via OAuth. Once connected, the tab shows service-specific controls.
- A `+` button to add new service tabs (future: Slack, Calendar, etc.)

**OAuth Flow:**
- Owner clicks "Connect Gmail" → GUI redirects to Google OAuth consent screen → user grants access → Google redirects back to `http://localhost:<port>/oauth/gmail/callback` → Hub saves the OAuth tokens (access token + refresh token) to DB/config, encrypted at rest
- Owner clicks "Connect GitHub" → GUI redirects to GitHub OAuth App authorization → user grants access → GitHub redirects back to `http://localhost:<port>/oauth/github/callback` → Hub saves the OAuth token
- Tokens are stored encrypted. The Hub uses them for fetching data (Gmail) or managing repo access (GitHub).

**Implement:**
- `src/gui/routes.ts` — mount GUI routes on the Hono server:
  - `GET /` — serve the GUI (single-page app)
  - `GET /oauth/:source/start` — initiate OAuth flow (redirect to provider)
  - `GET /oauth/:source/callback` — handle OAuth callback, save tokens
- `src/gui/frontend/` — lightweight frontend (React or Preact):
  - Tab bar component — one tab per service, `+` to add
  - Each service tab has sections based on the service type

**Service Tabs:**

- **Gmail tab** (when connected):
  - **Status**: connected as `user@gmail.com`, last synced at X, sync interval
  - **Access control** — toggle-based options that map to manifest operators internally:
    - **Time boundary**: "Only emails from the last N days" / "Only future emails" / "All emails" → sets `pull` boundary `after`
    - **Strip sender info**: toggle on → adds `select` that excludes `author_name`, `author_email`, `participants` from `data`
    - **Strip email body**: toggle on → adds `select` that excludes `body` (apps only see subject, labels, timestamp)
    - **Redact sensitive data**: toggle on → adds `transform { kind: "redact" }` for SSN, credit card, phone number patterns in `body`
    - **Truncate body**: toggle on + max length slider → adds `transform { kind: "truncate", max_length }` on `body`
    - **Label filter**: checkboxes for which labels are accessible (inbox, starred, etc.), excluded labels (spam, trash) → sets `pull` boundary labels
    - **Allow outbound actions**: toggle per action type — "Can draft emails", "Can send emails", "Can reply to emails" → controls whether `stage` operator is permitted
  - Each combination of toggles generates a manifest behind the scenes. When the owner changes a toggle, the Hub regenerates and stores the updated manifest in the `manifests` table.
  - **Default manifests** — pre-built manifests for common Gmail access patterns. The GUI offers these as presets the owner can select and customize:
    - **"Read-only, recent emails"** — pull emails from last 7 days, select title/body/labels/timestamp, redact SSNs. No outbound actions.
      ```
      pull { source: "gmail", type: "email" } → select { fields: ["title", "body", "labels", "timestamp"] } → transform { kind: "redact", field: "body", pattern: "SSN" }
      ```
    - **"Metadata only"** — pull emails, strip body and sender info. Apps only see subject, labels, timestamp.
      ```
      pull { source: "gmail", type: "email" } → select { fields: ["title", "labels", "timestamp"] }
      ```
    - **"Full access with redaction"** — pull all emails, keep all fields, redact sensitive patterns (SSN, credit card, phone), truncate body to 5000 chars.
      ```
      pull { source: "gmail", type: "email" } → transform { kind: "redact", field: "body", pattern: "sensitive" } → transform { kind: "truncate", field: "body", max_length: 5000 }
      ```
    - **"Email drafting"** — allows apps to propose email drafts for owner review.
      ```
      stage { action_type: "draft_email", requires_approval: true }
      ```
  - Owner can start from a preset and then adjust toggles to customize. The GUI regenerates the manifest on each change.
  - **Staging queue**: list of pending email drafts proposed by apps. Owner can preview the draft, approve (sends via Gmail API), or reject.
  - **Recent activity**: recent pulls and actions from audit log for this source

- **GitHub tab** (when connected):
  - **Status**: connected as `owner-username`, agent account: `agent-username`
  - **Access control**: list of repos, toggle which repos the agent account can access, permission level per repo (read/write). Changes call the GitHub collaborator API (Step 10) to grant/revoke.
  - **Recent activity**: recent pulls from audit log for this source

- **Settings tab**:
  - **API keys**: generate new keys for apps, view existing keys, revoke keys
  - **Audit log**: full data movement history across all sources
  - **Encryption**: master key status

**Dependencies:** `react` (or `preact`), bundled with `vite`

**Validate:**
- GUI loads at `http://localhost:<port>/`
- Gmail tab: click "Connect" → OAuth flow → redirects back → tab shows connected status with user email
- GitHub tab: click "Connect" → OAuth flow → redirects back → tab shows connected status
- Gmail tab: toggle label access → manifest updated in DB
- GitHub tab: toggle repo access → collaborator API called
- Staging: pending action appears → approve → connector executes → status updates to committed
- Settings: generate API key → key appears in list → can be used with `/app/v1/pull`

---

## Step 13: End-to-End Integration Tests

**What:** Full integration tests that simulate the V1 walkthrough: pull data → propose action → approve via GUI.

**Implement:**
- `tests/e2e/gmail-recent-readonly.test.ts`:
  - Use "Read-only, recent emails" preset manifest
  - Pull via App API → verify only `title`, `body`, `labels`, `timestamp` in response (no sender info)
  - Verify SSNs in body are redacted
  - Verify audit log entry created
- `tests/e2e/gmail-metadata-only.test.ts`:
  - Use "Metadata only" preset manifest
  - Pull via App API → verify only `title`, `labels`, `timestamp` returned
  - Verify `body`, `author_name`, `author_email`, `participants` are NOT present
- `tests/e2e/gmail-full-access-redacted.test.ts`:
  - Use "Full access with redaction" preset manifest
  - Pull via App API → verify all fields present
  - Verify sensitive patterns (SSN, credit card, phone) redacted in body
  - Verify body truncated to 5000 chars
- `tests/e2e/gmail-staged-action.test.ts`:
  - Use "Email drafting" preset manifest
  - Propose a draft email via App API → verify staging row created with `pending` status
  - Approve action (simulate DB write) → verify Gmail connector's `executeAction` was called
  - Verify audit log entries: `action_proposed` → `action_approved` → `action_committed`
- `tests/e2e/gmail-cache-sync.test.ts`:
  - Gmail sync runs → `cached_data` populated
  - Pull via App API → served from cache, Gmail API NOT called
  - Second sync → only new emails fetched

**Validate:**
- All e2e tests pass
- Each test is self-contained (starts fresh server, fresh DB)
- Tests verify both the HTTP response and side effects (DB state, audit log)

---

## Step 14: PersonalDataHub OpenClaw Extension

**What:** Create an OpenClaw extension called **PersonalDataHub** that intelligently interacts with the PersonalDataHub. The extension understands natural language queries about the user's personal data and translates them into the right sequence of PersonalDataHub API calls (pull, propose). It handles multi-step workflows — e.g., "Collect all unanswered emails since this morning and draft responses" becomes: pull emails → filter unanswered → for each email, propose a draft reply via staging.

**Implement:**
- `packages/personaldatahub/` (OpenClaw extension):
  - `src/index.ts` — register the extension with OpenClaw
  - `src/hub-client.ts` — thin HTTP client wrapping the 2 PersonalDataHub App API endpoints:
    - `pull(source, type?, params?, purpose)` → `POST /app/v1/pull`
    - `propose(source, actionType, actionData, purpose)` → `POST /app/v1/propose`
    - Reads API key from config, attaches `Authorization: Bearer pk_xxx`
    - The extension does not know about manifests or policies — it simply sends requests with a `purpose` string. The Hub resolves the policy internally.
  - `src/tools.ts` — registers tools that OpenClaw's agent can call:
    - `personal_data_pull` — pull data from a source through PersonalDataHub. Agent must provide a `purpose`.
    - `personal_data_propose` — propose an outbound action through PersonalDataHub staging. Agent must provide a `purpose`.
  - `src/prompts.ts` — system prompt that teaches the agent how to work with personal data:
    - Available sources (gmail, github) and what data types each provides
    - Every request must include a clear `purpose` explaining why the data is needed
    - How to compose multi-step workflows (pull → analyze → propose)
    - That outbound actions go through staging (owner must approve)
    - That data may be redacted/filtered based on owner's policy settings

**Example workflows the extension handles:**

All examples show the actual JSON payloads sent to the REST API. `params.query` uses Gmail search syntax (the Hub connector passes it directly to Gmail's `q` parameter). The Hub automatically applies boundary constraints (e.g., date cutoff) on top of the query. `params.limit` controls max results returned.

1. **"Collect all unanswered emails since this morning and draft responses"**
   - POST `/app/v1/pull`:
     ```json
     { "source": "gmail", "type": "email", "params": { "query": "is:unread newer_than:1d" }, "purpose": "Collect unanswered emails to draft responses" }
     ```
   - Agent analyzes each email, generates a draft reply
   - For each draft, POST `/app/v1/propose`:
     ```json
     { "source": "gmail", "action_type": "draft_email", "action_data": { "to": "alice@company.com", "subject": "Re: Q4 Report", "body": "Thanks Alice, the numbers look good." , "in_reply_to": "msg_abc123" }, "purpose": "Draft reply to unanswered email from Alice about Q4 report" }
     ```
   - Owner reviews drafts in PersonalDataHub GUI staging queue

2. **"Summarize my GitHub issues for this week"**
   - Agent knows GitHub goes through agent's own credentials (not PersonalDataHub pull)
   - Uses its own GitHub access to read issues from allowed repos
   - Summarizes and presents to user

3. **"Find emails from Alice about the Q4 report and draft a follow-up"**
   - POST `/app/v1/pull`:
     ```json
     { "source": "gmail", "type": "email", "params": { "query": "from:alice Q4 report" }, "purpose": "Find emails from Alice about Q4 report to draft follow-up" }
     ```
   - Agent reads the email thread, composes a follow-up
   - POST `/app/v1/propose`:
     ```json
     { "source": "gmail", "action_type": "draft_email", "action_data": { "to": "alice@company.com", "subject": "Re: Q4 Report", "body": "..." }, "purpose": "Draft follow-up to Alice re Q4 report" }
     ```

4. **"Set up a daily digest of my unread emails"**
   - POST `/app/v1/pull`:
     ```json
     { "source": "gmail", "type": "email", "params": { "query": "is:unread" }, "purpose": "Fetch unread emails for daily digest summary" }
     ```
   - Agent formats a summary digest
   - Returns the digest to the user (no staging needed — read-only)

**Validate:**
- Unit test: `hub-client.pull()` sends correct HTTP request with `purpose` field to `POST /app/v1/pull`
- Unit test: `hub-client.propose()` sends correct request with `purpose` field to `POST /app/v1/propose`
- Integration test: extension pulls emails → audit log contains the purpose string
- Integration test: extension proposes a draft → staging row created, audit log contains the purpose
- Manual: load extension in OpenClaw → ask "show me my recent emails" → agent calls `personal_data_pull` with purpose → returns results

---

## Implementation Order Summary

```
Step 1:  Project scaffolding                    ← no dependencies
Step 2:  Configuration system                   ← yaml, zod
Step 3:  Database and schema                    ← better-sqlite3, config
Step 4:  DataRow type + connector interface      ← types only
Step 5:  Manifest parser                        ← standalone
Step 6:  Operators (pull, select, filter,       ← connector types, db
         transform, stage, store)
Step 7:  Pipeline engine                        ← operators, manifest
Step 8:  Audit log                              ← db
Step 9:  HTTP server + App API                  ← pipeline, audit
Step 10: GitHub connector                       ← octokit, access control
Step 11: Gmail connector                        ← googleapis, connector types
Step 12: Owner GUI                              ← server, db, connectors, manifests
Step 13: End-to-end tests                       ← everything
Step 14: PersonalDataHub extension              ← App API client, OpenClaw tools
```

Steps 4, 5, and 8 have no cross-dependencies and can be developed in parallel.
Steps 10 and 11 (connectors) can be developed in parallel.

---

*Plan created: 2026-02-21*
